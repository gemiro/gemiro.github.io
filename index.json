[
{
	"uri": "/kubernetes/applications/metallb/",
	"title": "Kubernetes LoadBalancer, MetalLB",
	"tags": [],
	"description": "",
	"content": " 소개 MetalLB는 bare metal Kubernetes를 위한 Load-balancer 구현체이다. Kubernetes를 설치하고 어플리케이션을 배포하고 외부로 어플리케이션을 노출 시키기 위해서는 Service를 사용하게 된다. 이때 Service의 ServiceType을 선택할 수 있는데 ClusterIP(default), NodePort, LoadBalancer, ExternalName이 있다.\nbare metal Kubernetes ServiceType을 LoadBalancer로 선택하여 배포를 하면 Service의 EXTERNAL-IP 부분이 Pending 상태로 머물고 IP가 할당되지 않는다. MetalLB는 bare metal Kubernetes에서 ServiceType이 LoadBalancer 일 경우 EXTERNAL-IP를 할당해 주는 역할을 한다.\n사전 준비  쿠버네티스 설치하기 Helm 설치 및 사용 할당 할 IP 대역 마련(예: 공유기 Network 대역)  설치 MetalLB에서 Tutorial을 참조하면 BGP on Minukube와 Layer 2 mode tutorial 두 가지 방식을 설명하고 있다. 본 문에서는 Layer 2 mode 방식으로 설치 및 사용법을 보인다.\nMetalLB에서 설치 두 가지 설치 방법을 보이는데 1. 아래 명령어를 통해 manifest를 적용하여 MetalLB를 설치하는 방법\nkubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/tutorial-2.yaml\n helm을 사용하여 설치하는 방법을 알려주고 있다. helm install --name metallb stable/metallb  가장 쉬운 방법은 helm을 사용하여 설치하는 방법이 가장 쉽기 때문에 본 문에서는 helm을 사용하여 설치한다.\nmetallb helm chart git hub에 접속하면 values.yaml이 있다. 해당 파일을 master 노드 home 폴더에 복사한다.\nmkdir metallb cd metallb wget https://raw.githubusercontent.com/helm/charts/master/stable/metallb/values.yaml  그리고 values.yaml을 수정하는데 아래와 같이 address-pools 부분의 주석을 해제하고 Network 대역 중 할당할 범위를 addresses 부분과 같이 표시한다.\n... # available options. configInline: # Example ARP Configuration address-pools: - name: default protocol: layer2 addresses: - 192.168.1.200-192.168.1.250 ...  value.yaml을 수정하고 저장한 이후 아래 명령어를 통해 설치를 진행한다. helm install --name metallb --namespace metallb-system -f values.yaml stable/metallb\n설치 이후 아래 명령어를 통해 설치 결과를 확인하면, 아래와 같이 한 개의 controller와 slave 노드 개수 만큼의 speaker가 생성된 것을 알 수 있다.\nkubectl get pods -n metallb-system # NAME READY STATUS RESTARTS AGE # metallb-controller-776cdd69dc-drhgp 1/1 Running 1 1d # metallb-speaker-ftb4n 1/1 Running 1 1d # metallb-speaker-lgqtj 1/1 Running 1 1d  테스트 본 문에서는 간단하게 MetalLB에서 제공하는 nginx yaml을 사용하여 LoadBalancer가 적용된 manifest를 적용해 본다.\nkubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/tutorial-2.yaml\npod 가 정상적으로 배포 된 것을 알 수 있다.\nkubectl get po # NAME READY STATUS RESTARTS AGE # nginx-5cd9c7f879-vmzkz 1/1 Running 0 20s  service의 경우 EXTERNAL-IP부분이 위에서 설정한 Pool 범위에서 할당 되는 것을 알 수 있다.\nkubectl get svc # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # nginx LoadBalancer 10.106.212.89 192.168.1.203 80:31342/TCP 50s  같은 네트워크 대역에 있는 PC에서 브라우저를 열고 http://192.168.1.203으로 접속을 해보면 Welcome to nginx! 라는 구문을 볼 수 있다.\n"
},
{
	"uri": "/kubernetes/installation/",
	"title": "Centos7에 쿠버네티스 설치하기",
	"tags": [],
	"description": "",
	"content": " 사전 준비  Centos7이 설치된 PC 2대 준비 각각 공유기에 Lan 선이 연결되고 인터넷이 가능한 상태  설치 과정 ifconfig 가 활성화 되어 있지 않다면 아래 명령어를 통해 net-tools 설치 yum install net-tools (yum update 이후)\n\r접근 편의를 위하여 ssh 서버를 설치한다. yum install openssh-server openssh-clients openssh-askpass (yum update 이후)\n\r두 PC의 NetworkManager와 firewalld를 정지시키고 Reboot 이후에도 자동 실행되지 않도록 disable 시킨다.\n# root user or sudo systemctl disable NetworkManager systemctl stop NetworkManager systemctl disable firewalld systemctl stop firewalld  Docker 설치 (모든 PC) Reference: Installing Docker CE\nDocker 설치를 위해서 Repository를 셋업 한다.\n# root user or sudo yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo  Docker CE 설치\n# root user or sudo yum yum install docker-ce docker-ce-cli containerd.io systemctl start docker usermod -aG docker \u0026lt;your-user\u0026gt; //\u0026lt;your-user\u0026gt;부분을 사용자 명으로 변경  마지막 usermod는 non-root user가 docker명령어를 사용하기 위한 명령어로 사용자를 docker group에 포함시킨다.\nkubeadm, kubelet, kubectl 설치 (모든 PC) Reference: Installing kubeadm, kubelet and kubectl\n# root user or sudo cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kube* EOF swapoff -a setenforce 0 sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes systemctl enable --now kubelet cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system  Kubernetes 마스터 구성 (마스터 PC) Reference: Creating a single master cluster with kubeadm\n아래 과정을 통해 마스터를 설정하고 Cluster를 구성하기 위한 token을 통해 slave들을 등록한다.\n# root user or sudo kubeadm init --pod-network-cidr=10.244.0.0/16 # [init] Using Kubernetes version: vX.Y.Z # [preflight] Running pre-flight checks # [preflight] Pulling images required for setting up a Kubernetes cluster # [preflight] This might take a minute or two, depending on the speed of your internet connection # [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' # [kubelet-start] Writin... # ... # Your Kubernetes master has initialized successfully! # # To start using your cluster, you need to run the following as a regular user: # # mkdir -p $HOME/.kube # sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config # sudo chown $(id -u):$(id -g) $HOME/.kube/config # # You should now deploy a pod network to the cluster. # Run \u0026quot;kubectl apply -f [podnetwork].yaml\u0026quot; with one of the options listed at: # /docs/concepts/cluster-administration/addons/ # # You can now join any number of machines by running the following on each node # as root: # # kubeadm join \u0026lt;master-ip\u0026gt;:\u0026lt;master-port\u0026gt; --token \u0026lt;token\u0026gt; --discovery-token-ca-cert-hash sha256:\u0026lt;hash\u0026gt;  위 출력 내용 중 slave를 등록하기 전 non-root user가 kubectl을 접근하는 방법을 알려주고 있다. 해당 과정을 통해 regular user를 위한 kube config 파일을 HOME/.kube로 복사한다.\n# non-root user mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config kubectl get nodes # NAME STATUS ROLES AGE VERSION # master.k8s Ready master 1d v1.13.4  등록할 slave PC로 접속하여 위에서 발급 받은 token을 사용하여 master cluster에 join한다.\n# root urser or sudo kubeadm join \u0026lt;master-ip\u0026gt;:\u0026lt;master-port\u0026gt; --token \u0026lt;token\u0026gt; --discovery-token-ca-cert-hash sha256:\u0026lt;hash\u0026gt;  join이 제대로 등록 되면 master node로 돌아가 아래 명령어를 쳐보자.\nkubectl get nodes # NAME STATUS ROLES AGE VERSION # master.k8s Ready master 1d v1.13.4 # slave1.k8s Ready \u0026lt;none\u0026gt; 1d v1.13.4  만약 Not Ready 문구가 나오면 좀 더 기다려 본다. 만약 지속적으로 Not Ready 상태이면 해당 master와 slave간 통신이 제대로 되는지 또는 /etc/hosts 파일에 각각 NAME 이름으로 host를 등록하고 다시 시도해보자.\n완료 이후 Pod 네트워크 add-on을 설치해야 다양한 네트워크 플러그인들이 존재한다. 여기서는 Flannel를 설치한다. 이유는 나중 metallb 와 호환성이 좋기 때문이다. metallb는 baremetal 상에 설치된 kubernetes에서 서비스의 LoadBalancer External IP를 제공하는 기능을 하는 플러그인이다.\n아래 명령어 한 줄이면 Flannel 구성이 완료된다.\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml  기본적으로 Master에는 pods 스케쥴링이 되지 않는다. 만약 Slave 노드 자원이 부족하여 Master 노드에도 pods 스케쥴링이 되도록 하고 싶으면 다음 명령어를 통해 마스터에도 스케쥴링 가능하도록 한다. $ kubectl taint nodes --all node-role.kubernetes.io/master- node \u0026ldquo;test-01\u0026rdquo; untainted taint \u0026ldquo;node-role.kubernetes.io/master:\u0026rdquo; not found taint \u0026ldquo;node-role.kubernetes.io/master:\u0026rdquo; not found\n\r"
},
{
	"uri": "/spinnaker/installation/",
	"title": "Installations",
	"tags": [],
	"description": "",
	"content": " Title Contents \r"
},
{
	"uri": "/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": " Kubernetes Contents \rCentos7에 쿠버네티스 설치하기\r\r\rHelm 설치 및 사용\r\r\r어플리케이션(Helm)\r\r\r"
},
{
	"uri": "/spinnaker/",
	"title": "Spinnaker",
	"tags": [],
	"description": "",
	"content": " Title Contents \rInstallations\r\r\r"
},
{
	"uri": "/",
	"title": "The Study Room of Min-Jae Papa",
	"tags": [],
	"description": "",
	"content": " The Study Room of Min-Jae Papa Contents \rKubernetes\r\r\rSpinnaker\r\r\rSpring Boot\r\r\r"
},
{
	"uri": "/kubernetes/applications/nfs-provider/",
	"title": "Kubernetes NFS Dynamic Provisioner 구축",
	"tags": [],
	"description": "",
	"content": " 소개 하나의 컨테이너에 구성된 파일은 컨테이너 수명 주기를 따른다. 때문에 컨테이너가 죽고 삭제가 되면 컨테이너 내의 파일도 함께 삭제가 된다. 이는 State 정보를 보유하는 컨테이너가 운영중 문제가 발생하여 죽게되어 kubelet이 리스타트를 하게 된다면 데이터도 함께 손실된다는 것을 의미한다. 그리고 컨테이너 내의 파일은 기본적으로 컨테이너 내부에서만 사용이 가능하고 외부 컨테이너와 데이터를 공유 할 수 없다는 문제가 있다. Docker에서는 이를 해결하기 위해 volume 이라는 옵션으로 외부 파일 영역을 붙일 수 있다. Kubernetes에서도 똑 같이 Volume 이라는 추상화를 통해 위와 같은 문제들을 해결하고 있다. Kubernetes에서는 다음과 다양한 Volume 타입을 지원하고 있다. Types of Volumes\n지원 되는 Volume 타입들을 보면 몇몇 Volume들은 퍼블릭 클라우드에서 제공되는 Volume으로 프라이빗 클라우드의 경우 사용이 불가능하다. 프라이빗 클라우드의 경우 사용할 수 있는 대표적인 Volume으로 chphfs, cinder, iscsi, nfs 등이 있다.\n본 문서에서는 비교적 간단한 nfs를 사용하여 Volume을 구축해 본다. 그리고 Dynamic Provisioning을 통해 관리자 없이도 사용자 요구에 따라 자동으로 스토리지를 프로비저닝 해주는 기능까지 구축한다.\n사전 준비  쿠버네티스 설치하기 Helm 설치 및 사용 NFS 서버를 위한 PC  설치 NFS가 설치되어 있는 리눅스 서버가 있다면 아래 설치 부분을 Skip 한다. 만약 설치되어 있지 않다면 아래 과정을 통해 nfs 서버를 구축한다.\n# nfs 서버 설치 yum install nfs-utils nfs-utils-lib # nfs 스토리지 영역을 제공할 마운팅 지정 mkdir -p /nfs/data # nfs에 접속하는 사용자에대한 권한 설정 # Read,write 권한, 데이터 sync, root 권한을 요구하지 않도록 설정 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/exports /nfs/data *(rw,sync,no_root_squash) EOF # 방화벽이 disable 되어 있는지 확인 systemctl list-unit-files | grep fire # firewalld.service disabled # nfs 재시작 systemctl enable nfs systemctl restart nfs # 적용 확인 exportfs -v /nfs/data \u0026lt;world\u0026gt;(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash)  helm 차트 중 nfs-client-provisioner 차트를 사용하면 Dynamic Provisioning을 바로 구축이 가능하다.\nnfs.server 부분에 nfs 서버 주소를 기입하고 아래 명령어를 통해 nfs-client-provisioner를 구축한다.\nhelm install --name nfs --set nfs.server=x.x.x.x,nfs.path=/nfs/data stable/nfs-client-provisioner\n구축 이후 helm status nfs 명령어를 통해 구축 결과를 확인할 수 있다. 아래 내용중 StorageClass 부분을 통해 사용자는 YAML파일에 볼륨 부분에 StorageClass를 nfs-client를 주게 되면 자동으로 PersistentVolume과 PersistentVolumeClaim 이 생성된다.\nhelm status nfs # LAST DEPLOYED: Sat Mar 16 17:35:06 2019 # NAMESPACE: default # STATUS: DEPLOYED # # RESOURCES: # ==\u0026gt; v1/ClusterRole # NAME AGE # nfs-nfs-client-provisioner-runner 26d # # ==\u0026gt; v1/ClusterRoleBinding # NAME AGE # run-nfs-nfs-client-provisioner 26d # # ==\u0026gt; v1/Deployment # NAME READY UP-TO-DATE AVAILABLE AGE # nfs-nfs-client-provisioner 1/1 1 1 26d # # ==\u0026gt; v1/Pod(related) # NAME READY STATUS RESTARTS AGE # nfs-nfs-client-provisioner-57588997f-7c78l 1/1 Running 0 26d # # ==\u0026gt; v1/Role # NAME AGE # leader-locking-nfs-nfs-client-provisioner 26d # # ==\u0026gt; v1/RoleBinding # NAME AGE # leader-locking-nfs-nfs-client-provisioner 26d # # ==\u0026gt; v1/ServiceAccount # NAME SECRETS AGE # nfs-nfs-client-provisioner 1 26d # # ==\u0026gt; v1/StorageClass # NAME PROVISIONER AGE # nfs-client cluster.local/nfs-nfs-client-provisioner 26d  실행 nfs-client-provisioner이 구축이 되었다면 이제 PersistentVolumeClaim에 storageClassName을 지정하면 수동으로 PersistentVolume 구축 없이도 자동으로 지정된 볼륨이 생성할 수 있다. 간단하게 테스트 해보기 위해 PersistentVolumeClaim에 yaml 파일을 작성하고 kubectl create 명령으로 pvc를 생성해본다.\ncat \u0026lt;\u0026lt;EOF \u0026gt; ./pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: claim1 spec: accessModes: - ReadWriteOnce storageClassName: nfs-client resources: requests: storage: 1Gi EOF kubectl create -f pvc.yaml  끝으로 PersistentVolumeClaim과 자동으로 생성된 PersistentVolume을 확인한다.\nkubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE claim1 Bound pvc-5e0ac82f-5c6b-11e9-b099-b4b686df06b0 1Gi RWO nfs-client 4s kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-5e0ac82f-5c6b-11e9-b099-b4b686df06b0 1Gi RWO Delete Bound default/claim1 nfs-client 10s  kubectl get pv를 통해 나온 결과 처럼 PersistentVolumeClaim만 생성하였는데 자동으로 이에 해당하는 PersistentVolume이 생성된 것을 알 수 있다.\n"
},
{
	"uri": "/spring-boot/",
	"title": "Spring Boot",
	"tags": [],
	"description": "",
	"content": " Spring Boot Contents \r"
},
{
	"uri": "/kubernetes/helm/",
	"title": "Helm 설치 및 사용",
	"tags": [],
	"description": "",
	"content": " 소개 Kubernetes는 컨테이너 이미지를 배포하고 운영함에 있어 필요한 다양한 오케스트레이션 기능등을 지원하는 플랫폼이다. Kubernetes는 오케스트레이션 기능을 위해 YAML(또는 json) 파일을 사용하여 기능을 정의하고 관리한다. 하나의 컨테이너 이미지를 배포하기 위해서는 Kubernetes에서 제공하는 다양한 기능들을 이해하고 이를 YAML 파일을 사용하여 작성할 수 있어야하는데, 실제로 이미 잘 알려진 Mysql, MongoDB, Redis 등의 컨테이너 이미지를 사용하여 직접 YAML 파일을 작성하여 배포해 보면 이 일이 그렇게 말처럼 쉽지 않다는 것을 알 수 있다. 때문에 Kubernetes에서도 centos의 yum이나 ubuntu의 apt-get를 사용하면 쉽게 서비스를 설치하고 관리할 수 있는 것처럼 helm이라는 패키지 매니저가 나오게 되었다.\nhelm은 Kubernetes를 위한 패키지 매니저로써 helm 명령어를 통해 쉽게 어플리케이션을 배포하고 업데이트 삭제 등을 할 수 있다. helm은 어플리케이션을 배포하기 위해 chart repository를 사용하는데 이 repository에는 사용자 설정에 따라 kubernetes yaml을 동적으로 생성할 수 있는 chart 들이 저장되어 있다. chart는 직접 생성도 가능하지만 이미 Public으로 구축된 chart repository를 보면 대부분 필요한 어플리케이션등이 이미 마련되어 있다는 것을 알 수 있다.\n사전 준비  Kubernetes를 설치하고 master로 접속 kube config를 설정하여 kubectl 명령어를 사용할 수 있는 상태 Master node 에서 인터넷 접속이 가능한지 확인  설치 및 사용 helm 사이트에서 다양한 설치 방법을 제공하고 있는데 가장 쉽게 설치할 수 있는 방법은 스크립트를 다운받아 설치하는 것을 추천한다.\n아래 과정을 통해 스크립트를 다운받고 실행한다. 그러면 helm을 설치할 수 있는 환경이 구성된다.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get \u0026gt; get_helm.sh chmod 700 get_helm.sh ./get_helm.sh  환경이 구성되었으면 다음은 tiller를 설치해야 한다. tiller는 helm 서버의 일부분으로 kubernetes cluster에 pod로 올라간다. tiller는 기본적으로 kube-system namespace에 배포되고 tiller를 통해 배포되는 어플리케이션들은 실제로 다른 namespace에 배포되기 때문에 tiller에 적절한 cluster role이 설정되어야 한다. 아래 shell 명령어는 cluster-admin cluster role을 생성하고 이를 tiller에 binding 한 후 helm init을 통해 tiller를 배포하게 된다.\nkubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller helm init kubectl patch deploy --namespace kube-system tiller-deploy -p '{\u0026quot;spec\u0026quot;:{\u0026quot;template\u0026quot;:{\u0026quot;spec\u0026quot;:{\u0026quot;serviceAccount\u0026quot;:\u0026quot;tiller\u0026quot;}}}}'  배포가 완료 되었으면 아래 명령어를 통해 helm이 잘 동작하는지 확인한다.\n# helm repository 확인 : 기본적으로 stable repository만 검색이 된다. helm search # NAME CHART VERSION APP VERSION DESCRIPTION # incubator/artifactory 5.2.1 5.2.0 DEPRECATED Universal Repository Manager supporting all ma... # incubator/aws-alb-ingress-controller 0.1.4 v1.0.1 A Helm chart for AWS ALB Ingress Controller # incubator/azuremonitor-containers 0.5.0 2.0.0-3 Helm chart for deploying Azure Monitor container monitori... # incubator/burrow 0.3.3 0.17.1 Burrow is a permissionable smart contract machine # incubator/cassandra 0.10.4 3.11.3 Apache Cassandra is a free and open-source distributed da... # ... helm install --name my-release stable/mysql # NAME: my-release # LAST DEPLOYED: Sun Apr 7 16:51:10 2019 # NAMESPACE: default # STATUS: DEPLOYED # # RESOURCES: # ==\u0026gt; v1/ConfigMap # NAME DATA AGE # my-release-mysql-test 1 1s # # ==\u0026gt; v1/PersistentVolumeClaim # NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE # my-release-mysql Pending 1s # # ==\u0026gt; v1/Pod(related) # NAME READY STATUS RESTARTS AGE # my-release-mysql-796c4bdb6c-z7mqc 0/1 Pending 0 1s # # ==\u0026gt; v1/Secret # NAME TYPE DATA AGE # my-release-mysql Opaque 2 1s # # ==\u0026gt; v1/Service # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # my-release-mysql ClusterIP 10.107.170.208 \u0026lt;none\u0026gt; 3306/TCP 1s # # ==\u0026gt; v1beta1/Deployment # NAME READY UP-TO-DATE AVAILABLE AGE # my-release-mysql 0/1 1 0 1s # # # NOTES: # MySQL can be accessed via port 3306 on the following DNS name from within your cluster: # my-release-mysql.default.svc.cluster.local # # To get your root password run: # # MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default my-release-mysql -o jsonpath=\u0026quot;{.data.mysql-root-password}\u0026quot; | base64 --decode; echo) # # To connect to your database: # # 1. Run an Ubuntu pod that you can use as a client: # # kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il # # 2. Install the mysql client: # # $ apt-get update \u0026amp;\u0026amp; apt-get install mysql-client -y # # 3. Connect using the mysql cli, then provide your password: # $ mysql -h my-release-mysql -p # # To connect to your database directly from outside the K8s cluster: # MYSQL_HOST=127.0.0.1 # MYSQL_PORT=3306 # # # Execute the following command to route the connection: # kubectl port-forward svc/my-release-mysql 3306 # # mysql -h ${MYSQL_HOST} -P${MYSQL_PORT} -u root -p${MYSQL_ROOT_PASSWORD} # 위와 같은 결과가 출력되면 helm이 잘 작동한다는 것이다. # 아래 명령어는 배포된 mysql를 삭제하는 명령어이며 --purge 옵션은 배포된 my-release에 대한 name 히스토리를 삭제한다는 옵션이다. # 만약 --purge 옵션을 주지 않고 삭제한 경우 다음 배포할 때 my-release 이름으로 배포하면 에러가 난다. helm delete my-release --purge  팁 이미 helm으로 배포된 어플리케이션을 업그레이드 해야 하는 경우 아래 방법을 통해 쉽게 변경 내용을 적용할 수 있다.\n우선 배포된 어플리케이션의 values 값을 helm 명령어를 통해 추출한다. helm get values my-release \u0026gt; old_values.yaml\n그리고 변경이 필요한 부분을 변경하고 아래 명령어를 통해 변경된 old_values.yaml으로 업그레이드 한다.\nhelm upgrade -f old_values.yaml my-release stable/xxx\n"
},
{
	"uri": "/kubernetes/applications/",
	"title": "어플리케이션(Helm)",
	"tags": [],
	"description": "",
	"content": " Applications Contents \rKubernetes LoadBalancer, MetalLB\r\r\rKubernetes NFS Dynamic Provisioner 구축\r\r\r"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]